{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6C33C28wmzE",
        "outputId": "9d763b45-d142-4133-e27f-78d8902f5adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=031c27e1d6b19988b98c7a8da1086d9447113574eceddf8ef89834fb07832288\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "# Instale o PySpark no Google Colab\n",
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inicialize a sessão Spark\n",
        "spark = SparkSession.builder.appName(\"PySparkTest\").getOrCreate()\n",
        "\n",
        "# Dados e colunas\n",
        "data = [\n",
        "    (\"Alice\", 34, \"Data Scientist\"),\n",
        "    (\"Bob\", 45, \"Data Engineer\"),\n",
        "    (\"Cathy\", 29, \"Data Analyst\"),\n",
        "    (\"David\", 35, \"Data Scientist\")\n",
        "]\n",
        "columns = [\"Name\", \"Age\", \"Occupation\"]\n",
        "\n",
        "# Criação do DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QibCRVcSwx57",
        "outputId": "cb192aa0-6bdd-4090-ab0e-a1ad5ec7d954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+--------------+\n",
            "| Name|Age|    Occupation|\n",
            "+-----+---+--------------+\n",
            "|Alice| 34|Data Scientist|\n",
            "|  Bob| 45| Data Engineer|\n",
            "|Cathy| 29|  Data Analyst|\n",
            "|David| 35|Data Scientist|\n",
            "+-----+---+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionar as colunas \"Name\" e \"Age\"\n",
        "df_selected = df.select(\"Name\", \"Age\")\n",
        "\n",
        "# Filtrar as linhas onde a \"Age\" é maior que 30\n",
        "df_filtered = df_selected.filter(df_selected[\"Age\"] > 30)\n",
        "df_filtered.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymBYtSj2xCC1",
        "outputId": "7929b6e0-a77f-4aea-e2d5-171916743181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 34|\n",
            "|  Bob| 45|\n",
            "|David| 35|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Agrupar por \"Occupation\" e calcular a média de \"Age\"\n",
        "df_grouped = df.groupBy(\"Occupation\").agg(avg(\"Age\").alias(\"Average_Age\"))\n",
        "df_grouped.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I01Qp9yjxG0A",
        "outputId": "e2fa13fb-fe8f-491c-f34f-d54f7c4c7d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+\n",
            "|    Occupation|Average_Age|\n",
            "+--------------+-----------+\n",
            "|Data Scientist|       34.5|\n",
            "| Data Engineer|       45.0|\n",
            "|  Data Analyst|       29.0|\n",
            "+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordenar por média de idade em ordem decrescente\n",
        "df_sorted = df_grouped.orderBy(df_grouped[\"Average_Age\"].desc())\n",
        "df_sorted.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lghDRbe1xMlE",
        "outputId": "88fcc085-8663-4ea9-ce4b-ae1b421ba089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+\n",
            "|    Occupation|Average_Age|\n",
            "+--------------+-----------+\n",
            "| Data Engineer|       45.0|\n",
            "|Data Scientist|       34.5|\n",
            "|  Data Analyst|       29.0|\n",
            "+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Função Python para categorizar a idade\n",
        "def categorize_age(age):\n",
        "    if age < 30:\n",
        "        return \"Jovem\"\n",
        "    elif 30 <= age <= 40:\n",
        "        return \"Adulto\"\n",
        "    else:\n",
        "        return \"Senior\"\n",
        "\n",
        "# Converter a função em uma UDF\n",
        "categorize_age_udf = udf(categorize_age, StringType())\n",
        "\n",
        "# Aplicar a UDF no DataFrame\n",
        "df_with_category = df.withColumn(\"Age_Category\", categorize_age_udf(df[\"Age\"]))\n",
        "df_with_category.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8_xFe7qxTPc",
        "outputId": "4c36488c-e0a4-4ab4-e411-d5ddd793c30c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+--------------+------------+\n",
            "| Name|Age|    Occupation|Age_Category|\n",
            "+-----+---+--------------+------------+\n",
            "|Alice| 34|Data Scientist|      Adulto|\n",
            "|  Bob| 45| Data Engineer|      Senior|\n",
            "|Cathy| 29|  Data Analyst|       Jovem|\n",
            "|David| 35|Data Scientist|      Adulto|\n",
            "+-----+---+--------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Definir a janela\n",
        "windowSpec = Window.partitionBy(\"Occupation\")\n",
        "\n",
        "# Calcular a média de idade por \"Occupation\" e a diferença de idade\n",
        "df_with_window = df.withColumn(\"Avg_Age_Occupation\", avg(\"Age\").over(windowSpec))\n",
        "df_with_window = df_with_window.withColumn(\"Age_Difference\", df_with_window[\"Age\"] - df_with_window[\"Avg_Age_Occupation\"])\n",
        "df_with_window.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8YOtMthxX0-",
        "outputId": "4138e2d3-47d8-45f8-83a7-399f7ae51683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+--------------+------------------+--------------+\n",
            "| Name|Age|    Occupation|Avg_Age_Occupation|Age_Difference|\n",
            "+-----+---+--------------+------------------+--------------+\n",
            "|Cathy| 29|  Data Analyst|              29.0|           0.0|\n",
            "|  Bob| 45| Data Engineer|              45.0|           0.0|\n",
            "|Alice| 34|Data Scientist|              34.5|          -0.5|\n",
            "|David| 35|Data Scientist|              34.5|           0.5|\n",
            "+-----+---+--------------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de particionamento por coluna \"Occupation\"\n",
        "df_partitioned = df.repartition(3, \"Occupation\")\n",
        "df_partitioned.write.partitionBy(\"Occupation\").parquet(\"/content/occupation_partitioned.parquet\")\n"
      ],
      "metadata": {
        "id": "ljMWycyKxc9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Criar DataFrames para o join\n",
        "small_df = spark.createDataFrame([(\"Data Scientist\", \"DS\"), (\"Data Engineer\", \"DE\")], [\"Occupation\", \"Abbreviation\"])\n",
        "large_df = df\n",
        "\n",
        "# Aplicar o Broadcast Join\n",
        "result_df = large_df.join(broadcast(small_df), \"Occupation\")\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcKp23QRxiAf",
        "outputId": "d5e6c797-86a7-46ca-b1cd-484f0729f9b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----+---+------------+\n",
            "|    Occupation| Name|Age|Abbreviation|\n",
            "+--------------+-----+---+------------+\n",
            "|Data Scientist|Alice| 34|          DS|\n",
            "| Data Engineer|  Bob| 45|          DE|\n",
            "|Data Scientist|David| 35|          DS|\n",
            "+--------------+-----+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integração com Outras Tecnologias\n",
        "\n",
        "**Leitura de um arquivo CSV**\n",
        "\n",
        "Para ler um arquivo CSV em PySpark, usamos o método read.csv() da classe SparkSession. Este método permite carregar dados de um arquivo CSV em um DataFrame do PySpark, facilitando o processamento e a análise de grandes volumes de dados.\n",
        "\n",
        "Atribuição: \\\\\n",
        "1) *header=True*: Informa ao PySpark que o primeiro registro no arquivo CSV é o cabeçalho. \\\\\n",
        "2) *inferSchema=True*: Solicita ao PySpark que infira automaticamente o tipo de dado de cada coluna (e.g., inteiro, string, etc.)."
      ],
      "metadata": {
        "id": "YJrcuHssDo0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leitura de um arquivo CSV no Google Colab\n",
        "df_csv = spark.read.csv(\"/content/sample.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Mostrar o conteúdo do DataFrame\n",
        "df_csv.show()\n"
      ],
      "metadata": {
        "id": "_NTUE0zeEGNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Escrita em formato Parquet**\n",
        "\n",
        "O formato Parquet é um formato de armazenamento em colunas que é eficiente tanto em termos de espaço quanto de velocidade de leitura/escrita. O PySpark oferece suporte para a escrita de DataFrames em Parquet usando o método *write.parquet()*."
      ],
      "metadata": {
        "id": "buiH6UeVE0EB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escrita do DataFrame em formato Parquet\n",
        "df_csv.write.parquet(\"/content/output.parquet\")\n"
      ],
      "metadata": {
        "id": "JzLSgbZGD2fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integração do PySpark com o Hadoop HDFS**\n",
        "\n",
        "O Hadoop Distributed File System (HDFS) é um sistema de arquivos distribuído utilizado em ambientes Hadoop para armazenar grandes volumes de dados. O PySpark se integra perfeitamente ao HDFS, permitindo a leitura e escrita de dados diretamente no sistema distribuído.\n",
        "\n",
        "Para trabalhar com HDFS no PySpark, é necessário estar em um ambiente que tenha acesso a um cluster Hadoop, o que normalmente envolve configuração de um ambiente Hadoop ou a utilização de uma plataforma de nuvem que suporte Hadoop.\n",
        "\n",
        "**Leitura de um arquivo do HDFS**\n",
        "\n",
        "Atribuições:\n",
        "\n",
        "1) hdfs://namenode_host:port/: Esta parte da URL define o nome do nó (NameNode) e a porta do HDFS que será usada. Deve ser substituída pela URL real do seu ambiente Hadoop. \\\\\n",
        "2) O resto do caminho (/path/to/input.csv) especifica o local do arquivo no HDFS."
      ],
      "metadata": {
        "id": "Z6aSoXYBFGBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leitura de um arquivo CSV a partir do HDFS\n",
        "df_hdfs = spark.read.csv(\"hdfs://namenode_host:port/path/to/input.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Mostrar o conteúdo do DataFrame\n",
        "df_hdfs.show()\n"
      ],
      "metadata": {
        "id": "NLgBhbxkFDpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escrita de um arquivo de volta ao HDFS\n",
        "\n",
        "Depois de processar os dados, você pode salvá-los de volta no HDFS em diferentes formatos, como Parquet, CSV, etc. Aqui está um exemplo de como salvar os dados no HDFS em formato Parquet:\n",
        "\n"
      ],
      "metadata": {
        "id": "ydXyBHNJFhKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escrita do DataFrame no HDFS em formato Parquet\n",
        "df_hdfs.write.parquet(\"hdfs://namenode_host:port/path/to/output.parquet\")\n"
      ],
      "metadata": {
        "id": "eO1tJOEYFsh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema de Caso\n",
        "\n",
        "**Processamento de Logs**\n",
        "\n",
        "Neste problema, você temos um grande arquivo de log com as seguintes colunas: \"timestamp\", \"user_id\", \"action\". Cada linha representa uma ação realizada por um usuário em um determinado momento. A tarefa é processar este arquivo de log usando PySpark para extrair insights.\n",
        "\n",
        "**Carregue o arquivo de log em um DataFrame **\n",
        "\n",
        "Primeiro, carregamos o arquivo de log em um DataFrame usando a função *read.csv()*:\n",
        "\n",
        "\n",
        "Atribuições: \\\\\n",
        "1) timestamp: Representa o momento em que a ação foi realizada. \\\\\n",
        "2) user_id: Um identificador único para cada usuário. \\\\\n",
        "3) action: A ação realizada pelo usuário."
      ],
      "metadata": {
        "id": "yqoTcT87Ft-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o arquivo de log em um DataFrame\n",
        "df_log = spark.read.csv(\"/content/logfile.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Mostrar o conteúdo do DataFrame\n",
        "df_log.show()\n"
      ],
      "metadata": {
        "id": "Hvl2rd4jGa4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conte o número de ações realizadas por cada usuário**\n",
        "\n",
        "Usamos o método *groupBy()* para agrupar as ações pelo* user_id* e a função *count()* para contar o número de ações realizadas por cada usuário.\n",
        "\n",
        "Atribuições:\n",
        "\n",
        "1) *groupBy(\"user_id\"):* Agrupa os registros com base no identificador do usuário. \\\\\n",
        "2) *count():* Conta o número de ações realizadas por cada usuário."
      ],
      "metadata": {
        "id": "vZAOBCy3HA06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar o número de ações por usuário\n",
        "df_user_actions = df_log.groupBy(\"user_id\").count()\n",
        "\n",
        "# Mostrar o resultado\n",
        "df_user_actions.show()\n"
      ],
      "metadata": {
        "id": "7vtbanh8G_xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encontre os 10 usuários mais ativos**\n",
        "\n",
        "Depois de contar as ações, podemos ordenar os usuários pelo número de ações em ordem decrescente e selecionar os 10 mais ativos:\n",
        "\n",
        "Atribuições:\n",
        "\n",
        "1) *orderBy(df_user_actions[\"count\"].desc())*: Ordena os usuários pelo número de ações em ordem decrescente.\n",
        "\n",
        "2) *limit(10)*: Seleciona apenas os 10 primeiros usuários da lista."
      ],
      "metadata": {
        "id": "CG5nhsM7HbVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontrar os 10 usuários mais ativos\n",
        "df_top_users = df_user_actions.orderBy(df_user_actions[\"count\"].desc()).limit(10)\n",
        "\n",
        "# Mostrar os usuários mais ativos\n",
        "df_top_users.show()\n"
      ],
      "metadata": {
        "id": "DzHC5G_DG27Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salve o resultado em um arquivo CSV\n",
        "\n",
        "Finalmente, salvamos o resultado dos 10 usuários mais ativos em um arquivo CSV:"
      ],
      "metadata": {
        "id": "0j4nVEeZH2bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o resultado em um arquivo CSV\n",
        "df_top_users.write.csv(\"/content/top_users.csv\")\n"
      ],
      "metadata": {
        "id": "F0lS-c5BH3_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Referências**\n",
        "\n",
        "1) Zaharia, M., Chowdhury, M., Franklin, M. J., Shenker, S., & Stoica, I. (2010). \"Spark: Cluster computing with working sets.\" In Proceedings of the 2nd USENIX conference on Hot topics in cloud computing (Vol. 10, No. 10-10, p. 95).\n",
        "\n",
        "\n",
        "2) Armbrust, M., Das, T., & Xin, R. S. et al. (2015). \"Spark SQL: Relational Data Processing in Spark.\" In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (pp. 1383-1394).\n",
        "\n",
        "3) Karau, H., Warren, R., Wendell, P., & Zaharia, M. (2017). Learning Spark: Lightning-fast big data analysis. O'Reilly Media.\n",
        "\n",
        "\n",
        "4) White, T. (2012). Hadoop: The Definitive Guide. O'Reilly Media.\n",
        "\n",
        "\n",
        "5) Meng, X., Bradley, J., Yavuz, B., Sparks, E., Venkataraman, S., Liu, D., ... & Zaharia, M. (2016). \"MLlib: Machine learning in Apache Spark.\" Journal of Machine Learning Research, 17(1), 1235-1241.\n",
        "\n",
        "\n",
        "6) Guller, M. (2015). Big Data Analytics with Spark: A Practitioner’s Guide to Using Spark for Large Scale Data Analysis. Apress.\n"
      ],
      "metadata": {
        "id": "8ZQNpU-sJTb7"
      }
    }
  ]
}